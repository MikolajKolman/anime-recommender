{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAL_API:\n",
    "\n",
    "    USERS_ANIMELIST_URL = 'https://api.myanimelist.net/v2/users/{}/animelist?fields=list_status&limit=100'\n",
    "    # Lists with keys from animelist dictionaries of variables to be extracted.\n",
    "    list_entry_node_keys = ['id', 'title']\n",
    "    list_entry_list_status_keys = ['status', 'score', 'num_episodes_watched', 'updated_at']\n",
    "\n",
    "    def __init__(self, client_id: str) -> None:\n",
    "        self.CLIENT_ID = client_id\n",
    "        self.api_headers = {\n",
    "            'X-MAL-CLIENT-ID' : self.CLIENT_ID\n",
    "        }\n",
    "\n",
    "    # TODO thrash this - not used\n",
    "    def get_anime_ranking(self, request_parameters='?ranking_type=all') -> dict:\n",
    "        response = requests.get('https://api.myanimelist.net/v2/anime/ranking' + request_parameters, headers=self.api_headers)\n",
    "        return response.json()\n",
    "    \n",
    "    def __parse_list_entry(self, entry: dict) -> dict:\n",
    "        \"\"\"Parse an entry of animelist. Select important variables and merge dictionaries.\n",
    "\n",
    "        Args:\n",
    "            entry (dict): Anime entry in a animelist from the API.\n",
    "\n",
    "        Returns:\n",
    "            dict: Parsed entry of animelist as a single dictionary.\n",
    "        \"\"\"\n",
    "        node = entry['node']\n",
    "        node_parsed = {key: node[key] for key in self.list_entry_node_keys}\n",
    "        list_status = entry['list_status']\n",
    "        list_status_parsed = {key: list_status[key] for key in self.list_entry_list_status_keys}\n",
    "        # Not all entries have start and/or finished dates. Users have not started or finished some of the anime on their lists.\n",
    "        # If they don't exist add them with `None` values.\n",
    "        if 'start_date' in list_status.keys():\n",
    "            list_status_parsed['start_date'] = list_status['start_date']\n",
    "        else:\n",
    "            list_status_parsed['start_date'] = None\n",
    "        if 'finish_date' in list_status.keys():\n",
    "            list_status_parsed['finish_date'] = list_status['finish_date']\n",
    "        else:\n",
    "            list_status_parsed['finish_date'] = None\n",
    "        parsed_animelist = {**node_parsed, **list_status_parsed}\n",
    "        return parsed_animelist\n",
    "\n",
    "    def get_users_animelist(self, username: str) -> dict:\n",
    "        # End condition response['paging'] - no 'next' key in dictionary.\n",
    "        animelist_url = self.USERS_ANIMELIST_URL.format(username)\n",
    "        animelist = []\n",
    "        end_of_list = False\n",
    "        while not end_of_list:\n",
    "            animelist_page = requests.get(animelist_url, headers=self.api_headers).json()\n",
    "            # Add page to animelist list.\n",
    "            animelist.extend([self.__parse_list_entry(anime_entry) for anime_entry in animelist_page['data']])\n",
    "            # Change url to next page. Checking for end condition.\n",
    "            paging = animelist_page['paging']\n",
    "            if 'next' in paging.keys():\n",
    "                animelist_url = paging['next']\n",
    "            else:\n",
    "                end_of_list = True\n",
    "        return animelist\n",
    "        \n",
    "\n",
    "CLIENT_ID = '32df078edb7cabcd2eb77f026384e66b'\n",
    "mal_api = MAL_API(client_id=CLIENT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tet = mal_api.get_users_animelist('mateuszvaper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 23283,\n",
       " 'title': 'Zankyou no Terror',\n",
       " 'status': 'plan_to_watch',\n",
       " 'score': 0,\n",
       " 'num_episodes_watched': 0,\n",
       " 'updated_at': '2021-10-30T15:56:14+00:00',\n",
       " 'start_date': None,\n",
       " 'finish_date': None}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tet[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'headers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m api_url \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhttps://api.myanimelist.net/v2/users/mateuszvaper/animelist?fields=list_status\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(api_url, headers\u001b[39m=\u001b[39mheaders)\n\u001b[1;32m      4\u001b[0m resp \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()\n\u001b[1;32m      5\u001b[0m resp\n",
      "\u001b[0;31mNameError\u001b[0m: name 'headers' is not defined"
     ]
    }
   ],
   "source": [
    "api_url = \n",
    "\n",
    "response = requests.get(api_url, headers=headers)\n",
    "resp = response.json()\n",
    "resp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "\n",
    "I was considering supplying usernames with all members from top 100 anime. However a list of members of an anime only contains 7500 users, that have recently updated a this anime on their list. Therefore I will only use the random sample from the `users.php` page of recently active users."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping usernames by users page with recently active users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_page_url = 'https://myanimelist.net/users.php'\n",
    "user_names_filename = 'user_names.txt'\n",
    "\n",
    "with open(user_names_filename, 'a') as usernames_file:\n",
    "    # TODO add loop with number of usernames to scrape\n",
    "    users_soup = BeautifulSoup(requests.get(users_page_url).text, 'html.parser')\n",
    "    user_names = [user_soup.find('a').get_text() for user_soup in users_soup.find('td').find_all('td')]\n",
    "    for username in user_names:\n",
    "        print(username, file=usernames_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting users anime lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
